- 웹 크롤러 : 로봇, 스파이더라고도 부름 , 웹에 새로 올라오거나 갱신된 컨텐츠를 찾아내는것이 주 목적( 이미지 비디오 파일 등등)
    - 검색 엔진 인덱싱 :  웹 페이지를 모아 검색엔진을 위한 로컬 인덱스를 만듬 ,가장 보편적, ex) 구글봇(구글 검색엔진)
    - 웹 아카이핑 : 나중에 사용할 목적으로 장기 보관하기 위해 웹에서 정보를 모음 , ex) 미국 국회 도서관, EU 웹 아카이브
    - 웹 마이닝 : 인터넷에서 유용한 지식을 도출 가능 ex)주주총회자료,연 보고서 로 기업의 핵심사업 방향을 알아냄
    - 웹 모니터링 : 저작권이나 상표권이 침해되는 사례 모니터랑 ex) 디지마크
- 문제 및 범위
    - URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를
    다운로드한다 → 다운받은 웹 페이지에서 URL들을 추출한다. → 반복
    - 규모 확장성 - 병행 , 안정성 - 악성이나 빈 페이지 등 , 예절 - 너무 자주 요청 보내지 않기 , 확장성 - 새로운 콘텐츠 지원이 쉬워야함
    - 매달 10억개, qps = 10억 , 초당 400페이지, 최대qps  800, 평균크기 500k, 월 500 테라, 총 30PB
- 규모 확장성, 예의, 확장성, 안정성 등을 고려
    - 서버사이드 렌더링 - 동적 링크는 발견하기 어렵기에 파싱전에 서버 측 렌더링(동적 렌더링dynamic rendering) 적용 , 데이터 다중화 및 샤딩,수평적 규모 확장성(무상태 서버),가용성, 일관성,데이터 분석 솔루션 등 고려
- 작업 흐름
    
    ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/59a94395-18fc-496f-bddd-69dc0359c611/7c771e79-1ae1-492f-a87c-6688e198d145/image.png)
    
    - 시작 URL들을 미수집 URL 저장소에 저장
        - 많은 url을 탐색할 수 있는 시작 url을 고름, 접체를 부분집합으로 나누기- 나라별, 주제별
        - DFS vs BFS
            - directed graph , 페이지가 노드 , 하이퍼링크는 에지, 보통BFS 사용 ( 큐의 한쪽에는 탐색할 url을 넣고 한쪽으로는 꺼내기만 함)
            - 한페이지의 링크는 대부분 같은 서버인데, 병렬로 처리 시 과부하, 우선순위를 두어 처리 순서를 가지게 해야함
    - HTML 다운로더는는 미수집 URL 저장소에서 목록을 가져와서 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아내고, 해당 IP 주소로 접속하여 웹 페이지를 다운받음
        - 다운로드할 url(미수집url 저장소, FIFO 큐), 다운로드된 url
        - 다운받으려면 url을 ip주소로 변환하는 절차가 핋요함
        - HTTP 다운로더
            - HTTP 프로톸ㄹ을 통해 웹페이지르 ㄹ내려 받음
            - 로봇 제외 프로토콜 - Robots.txt는 웹사이트가 크롤러와 소통하는 표준적 방법 , 크롤러가 수집해도 되는 페이지 목록이 있음. 주기적으로 다시 다운받아 캐시에 보관.
            - 다운로더 성능 최적화 :
                - 분산 크로링 - 크롤링 작업을 여러 서버에 분산. 각각 여러 스레드르 ㄹ돌려 다운로드 작업 처리.
                - 도메인 이름 변환 결과 캐시 - 이름변환긴(DNS RESOLVER)는 성능 병목중 하나. 요청을 보내고 받는 작업이 동기임. 다른 스레드의 DNS 요청도 전부 블록되기에 주기적으로 도메인 이름과 IP를 캐시에 보관
                - 지역성- 크롤링  수ㅐㅇ 서버를 지역별로 분산
                - 짧은 타임아웃 - 응답이 없는 웹서버의 타임아웃시간을 짧게 설정
            - 다운로더 안정성
                - 안정해시 : 서버들에 부하는 분산할떄 적용 ( 제거 추가 용이)
                - 크롤링 상태 및 수집 데이터 저장 : 장애 복구가 쉽도록 크롤링 상태와 수집된 데이터를 지속저장장치에기록하여 중단났던 크롤링 재시작
                - 예외 처리 : 에러 처리
                - 데이터 검증
            - 다운로더 확장성
                - 새로운 콘텐츠도 지원할 수 있도록 설계 , PNG 다운로더, 웹모니터(저작권) emd
    - 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증
        - 콘텐츠 파서- 파싱 & 검증
        - 문제있는 콘텐츠 감지 및 회피
            - 중복건텐츠 - 해시, 체크섬
            - 거미 덫 - 무한굴레에 빠뜨리도록 설계된 웹, 무한디렉토리는 최대길이 제한, 알고리즘을 만들어내기는 까다롭기 때문에 수작업으로 확인하고 제외대상에 추가
            - 데이터 노이즈 - 가치없는 콘텐츠(광고, 스크립트코드,스팸 등) 제거 ( 스팸방지 컴포넌트)
    - 중복 콘텐츠인지 확인 (해당 페이지가 이미 저장소에 있는지)
        - 29% 중복, 해시값 비교
        - 저장소는 HTML 문서를 보관, 디스크 & 인기 있는 콘텐츠는 메모리 저장소
    - 없는 콘텐츠인 경우에는 저장소에 저장한 뒤 URL 추출기로 전달
        - URL 추출기 -HTML 페이지를 파싱하여 링크 추출,  상대 경로늕  절대 경로로
    - URL 추출기는 해당 HTML 페이지에서 링크를 추출, 골라낸 링크를 URL 필터로 전달
        - URL 필터 - 특정 콘텐츠 타입, 오류 , 제외 목록 등을 크롤링 대상에서 제외
    - 남은 URL만 중복 URL 판별( URL 저장소에 보관된 URL인지), URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소에도 전달
        - 방문 url - 블룸필터, 해시테이블 사용
        - 미수집 URL 저장소
            - 메모리 버퍼에 큐르 ㄹ두어 버퍼에 있느 ㄴ데이터르 ㄹ주기적으로 디스크에 기록하도록 하여 성능 병목지점(all 디스크) 처리
            - dos 공격으로 간주될 수 도 있기에 동일 웹 사이트에는 한번에 한페이지만 요청
            - 웹사이트의 호스트명과 작업 스레드 사이의 관계를 유지. → 별도 FIFO 큐를 가져 해당 큐에서ㅏㄴ 꺼냄
            - 큐 라우터(queue router): 같은 호스트에 속한 URL은 언제나 같은 큐로 가도록 보장한다
            - 매핑 테이블(mapping table): 호스트 이름과 큐사이의 관계를 보관
            - FIFO 큐: 같은 호스트에 속한 URL은 언제나 같은 큐
            - 큐 선택기(queue selector): 큐들을 순회하면서 큐에서 URL 지정된 작업 스레드에 전달하여 다운로드 하게 함
            - 작업 스레드(worker thread):  URL을 다운로드. 전달된 URL은 순차처리, 지연시간을 줄수있음
            - 우선순위→ 순위결정장치 : 페이지 랭크, 트래픽양, 갱신ㄴ빈도 등의 척도를 사용, 우선순위를 계산하여 우선순위별로 큐가 하나씩 할당, 큐선택기는 순위가 높은 큐에서 더 자주 꺼내게 함
            - 전면 큐 : 우선순위 / 후면큐 : 예의바른 큐
            
            ![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/59a94395-18fc-496f-bddd-69dc0359c611/6aedfcf9-2dcc-4e8c-a849-a1037c0f982d/image.png)
