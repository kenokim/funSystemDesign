**웹 크롤러는 로봇(robot) 또는 스파이더(spider)라고도 부른다.**

**검색엔진에서 널리 쓰는 기술이며, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적이다.**

**크롤러의 이용**

- 검색 엔진 인덱싱 (search engine indexing)
    - 크롤러의 가장 보편적인 용례
    - googlebot - 구글 검색 엔진이 사용하는 웹 크롤러
- 웹 아카이빙 (web archiving)
    - 장기보관하기 위해 웹에서 정보를 모으는 절차
    - 많은 국립 도서관이 크롤러를 돌려 웹 사이트를 아카이빙하고 있다.
    - 미국 국회 도서관
- 웹 마이닝 (web mining)
    - 웹 마이닝을 통해 인터넷에서 유용한 지식을 도울해 낼 수 있다.
    - 유명 금융 기업들은 크롤러를 사용해 주주총회 자료나 연차 보고서(annual report)를 다운받아 기업의 핵심 사업 방향을 알아내기도 한다.
- 웹 모니터링 (web monitoring)
    - 인터넷에서 저작권이나 상표권을 침해되는 사례를 모니터링 가능하다.
    - 디지마크사(Digimarc Company)는 웹 크롤러를 사용해서 해적판 저작물을 찾아내서 보고한다.

**웹 크롤러의 복잡도**

- 웹 크롤러가 처리해야하는 데이터의 규모에 따라 달라질 수 있다.

## 1단계 문제 이해 및 설계 범위 확정

**웹 크롤러의 기본 알고리즘**

1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
2. 다운받은 웹 페이지에서 URL들을 추출한다.
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복한다.

**웹 크롤러의 동작은 위처럼 간단하지 않다. 따라서 설계 진행 전 질문을 던져서 요구사항을 알아내고 설계 범위를 좁혀야한다.**

**크롤러 기능 요구사항을 명확히 하는 한편, 좋은 웹 크롤러가 만족 시켜야할 다은 같은 속성에 주의를 기울이는 것도 바람직하다.**

**웹 크롤러가 만족시켜야 할 속성**

- 규모 확장성
- 안정성 (robustness)
- 예절 (politeness)
- 확장성 (extensibility)

## 2단계 개략적인 설계안 제시 및 동의 구하기

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3b128aa8-2b95-401c-9089-9490077bf2a3/6ff37e7d-5907-4829-9046-6c6ebd2594e6/image.png)

**각 컴포넌트의 기능**

- 시작 URL 집합
    - 웹 크롤러가 크롤링을 시작하는 출발점
    - 예를 들면 대학의 도메인 이름이 붙은 모든 페이지의 URL을 시작으로 쓴다.
    - 크롤러가 가능한 한 많은 링크를 탐색할 수 있도록 하는 URL을 고르느느 것이 바람직할 것이다.
    - 보통 전체 URL 공간을 작은 부분 집합으로 나눈 전략 사용
    - 시작 URL로 무엇을 쓸 것이냐는 질문에 정답은 없다.
- 미수집 URL 저장소
    - 대부분의 현대적인 웹 크롤러는 크롤링 상태를 *다운로드할 URL*, *다운로드된 URL* 두 상태로 나눠 관리한다.
    - 이 중 다운로드할 URL을 저장 관리하는 컴포넌트를 미수집 URL 저장소(URL frontier)라고 부른다.
    - FIFO 큐
- HTML 다운로더
    - 인터넷에서 웹 페이지를 다운로드 하는 컴포넌트
    - 다운로드할 페이지의 URL은 미수집 URL 저장소가 제공
- 도메인 이름 변환기
    - 웹 페이지 다운을 위해 URL을 IP 주소로 변환하는 절차가 필요한데, URL에 대응되는 IP 주소를 알아내는 컴포넌트
- 콘텐츠 파서
    - 웹 페이지 다운로드 후 파싱(parsing)과 검증(validation) 절차가 필요하다.
    - 크롤링 서버 안에 구현시 크롤링 과정이 느려지게 되므로 독립된 컴포넌트로 구성
- 중복 콘텐츠인가?
    - 연구 결과에 따르면 29%가량의 웹 페이지 콘텐츠는 중복이다.
    - 데이터의 중복을 줄이고 데이터 처리에 소요되는 시간을 줄인다.
    - 웹 페이지의 해시값 비교를 통해 찾아내는 것이 효과적이다.
- 콘텐츠 저장소
    - HTML 문서를 보관하는 시스템
    - 본 설계안의 경우 디스크와 메모리를 동시에 사용하는 저장소를 택할 것이다.
- URL 추출기
    - HTML 페이지를 파싱하여 링크들을 골라내는 역할
    - 상대경로를 절대경로로 변환
- URL 필터
    - URL이 특정한 콘텐츠 타입/접속 시 오류 페이지/접근 제외 목록(deny list)인 경우 크롤링 대상에서 배제하는 역할을 한다.
- 이미 방문한 URL?
    - 이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있도록 하는 자료구조를 사용
    - 같은 URL을 여러번 처리하는 일을 방지
    - 블룸 필터(bloom filter)나 해시 테이블이 널리 쓰임
- URL 저장소
    - 이미 방문한 URL 보관하는 저장소

**웹 크롤러 작업 흐름**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3b128aa8-2b95-401c-9089-9490077bf2a3/8a76531f-420b-4850-ab6e-a393a4e6ff92/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3b128aa8-2b95-401c-9089-9490077bf2a3/f5cbe9ea-1a52-4c6f-bf23-28fb1c0a0ca4/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3b128aa8-2b95-401c-9089-9490077bf2a3/62f717e6-275e-4a13-8e04-07d479070f82/image.png)

## 3단계 상세 설계

**지금부터는 가장 중요한 컴포넌트와 그 구현 기술을 심도 있게 살펴보겠다.**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/3b128aa8-2b95-401c-9089-9490077bf2a3/91518d70-2d55-48f1-bd2b-54029eaddee0/image.png)

**DFS를 쓸 것인가, BFS를 쓸 것인가**

- 웹은 유향 그래프(directed graph)나 같다.
- 페이지 탐색시 DFS, BFS 알고리즘을 사용할 수 있다.
- 깊이 우선 탐색(depth-first search)를 사용하는 것은 좋은 선택이 아닐 수 있다. 왜냐하면 어느정도로 깊숙이 가게될지 가늠하기 어려워서이다.
- 따라서 웹 크롤러는 보통 너비 우선 탐색(breath-first search)을 사용한다. (FIFO 큐를 사용하는 알고리즘이다.)
- 해당 구현법에는 두 가지 문제가 있다.
    - 한 페이지에서 나오는 링크의 상당수는 같은 서버로 요청이 가게 되는데, 해당 서버에서 웹 페이지를 모두 병렬로 다운받게 된다면 해당 서버는 과부하에 걸리게 될 것이다. → 예의 없는(impolite) 크롤러
    - 표준적 BFS 알고리즘의 경우 우선순위가 없어 모든 웹 페이지를 공평하게 대우한다. 따라서 페이지 순위, 사용자 트랙피의 양, 업데이트 빈도 등 여러가지 척도를 통해 우선수누이를 구별하는것이 온당할 것이다.

**미수집 URL 저장소**

- 다운로드할 URL을 보관하는 장소
- 이 저장소를 잘 구현하면 예의(politeness)를 가춘 크롤러, URL 사이의 우선순위와 신선도(freshness)를 구별하는 크롤러를 구현할 수 있다.

미수집 URL 저장소의 구현 방법 중 중요한 것을 요약하면 다음과 같다.

- 예의
    - 웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것을 삼가야 한다.
    - 예의 바른 크롤러를 만드는 데 있어서 지켜야 할 한가지 원칙은 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청한다는 것이다.
- 우선순위
- 신선도
- 미수집 URL 저장소를 위한 지속성 저장장치

**HTML 다운로더**

HTTP 프로토콜을 통해 웹 페이지를 내려받는다.

로봇 제외 프로토콜(Robot Exclu-sion Protocol)

- Robots.txt
    - 웹 사이트가 크롤러와 소통하는 표준적인 방법
    - 웹 사이트를 긁어가기 전에 크롤러는 해당 파일에 나열된 규칙을 먼저 확인해야한다.

성능 최적화 기법

- 분산 크롤링
    - 성능을 높이기 위해 크롤링 작업을 여러 서버에 분산하는 방법
- 도메인 이름 변환 결과 캐시
    - 도메인 이르 변환기(DSN Resolver)는 크롤러 성능의 병목 중 하나.
    - DNS 요청을 보내고 결과를 받는 작업의 동기적 특성 때문
- 지역성
    - 크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법
    - 지역적ㅇ로 가까우면 다운로드 시간이 줄어들 것
- 짧은 타임아웃
    - 어떤 웹 서버는 아예 응답 하지 않거나 느리기 때문에 타임아웃을 두어 다운로드를 중단하고 다음 페이지로 넘어가는 것도 방법이다.

**안정성**

- 안정 해시(consistent hashing) : 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술
- 크롤링 상태 및 수집 데이터 저장 : 장애가 발생한 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해두는 것이 바람직함. 중단하였던 크롤링 재시작 가능
- 예외 처리(exception handling) : 대규모 시스템에서 에러(error)는 불가피할 뿐 아니라 흔하게 벌어지는 일이기 때문에 예외가 발생해도 전체 시스템이 중다노디는 일 업이 그 작업을 우아하게 이어나갈 수 있어야 한다.
- 데이터 검증(data validation) : 시스템 오류를 방지하기 위한 중요 수단 가운데 하나.

**확장성**

- 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 신경 써야 한다.

**문제 있는 콘텐츠 감지 및 회피**

중복이거나 의미 없는, 또는 유해한 콘텐츠를 어떻게 감지하고 시스템으로부터 차단할지

1. 중복 콘텐츠
    
    : 해시나 체크섬 활용하여 중복 콘텐츠 탐지
    
2. 거미 덫(spider trap)
    
    : 무한히 깊은 디렉토리 구조를 포함하는 링크의 경우 덫을 자동으로 피해가는 알고리즘을 만들어야하는데 쉽지않다. 따라서 탐색 대상에서 제외하거나 URL 필터 목록에 걸어두면 된다.
    
3. 데이터 노이즈
    
    : 광고나 스크립트 코드, 스팸 URL같이 도움될 것이 없는 콘텐츠는 가능하다면 제외해야한다.
    

## 4단계 마무리

추가적으로 논의해보면 좋을 것들

- 서버 측 렌더링(server-side rendering)
- 원치 않는 페이지 필터링
- 데이터베이스 다중화 및 샤딩
- 수평적 규모 확장성
- 가용성, 일관성, 안정성
- 데이터 분석 솔루션

---

## 질문 및 추가 공부 내용
