# ch06. 키-값 저장소 설계
## 이론
- CAP 정리: 현실 세계에서는 CP 또는 AP 시스템만 가능하다.
- Quorum 이란? 분산 시스템에서 의사결정을 내리기 위한 최소한의 동의수, 데이터 일관성을 보장하기 위해 사용할 수 있다.
  - Quorum 값을 통해 CAP 의 밸런싱을 할 수 있다.
- 일관성 모델

## Questions
- Q). 데이터베이스 복제에 대해, 쓰기 작업을 하나의 노드에서만 처리하는 것과 여러 노드에서 처리하는 구조의 차이점에 대해?

## 사례: redis replica & sentinel failover
- replicaof 입력 시 자기 데이터는 삭제되고 복제가 시작됨.
- redis sentinel 은 별도의 파드, 최소 3개 이상 띄우는게 권장됨. 홀수 개가 아니면 split brain 현상 발생 가능.
- sentinel monitor mymaster 127.0.0.1 6379 2 -> 이 값이 quorum, 이 값을 충족해야 failover 시작
- 클라이언트는 sentinel 에 요청하여 master 주소를 동적으로 알 수 있다.

### redis sentinel split brain 현상 예시
- 예를 들어, node 1, node 2 사이에 네트워크 에러가 발생할 경우 node 2 에 있는 sentinel 2대는 master 의 fail 을 quorum = 2를 만족하여 slave 를 master 로 승격한다.
- 즉, master 가 2개가 되는 상황이 발생한다.
![image](https://github.com/user-attachments/assets/a21f29e6-269a-4a7f-ab52-00703aafaa51)



## 일관성 (consistency)
- 데이터가 여러 노드에 복제된 경우, 어떤 노드에 요청을 하더라도 같은 결과가 나와야 일관성이 있다고 볼 수 있다.
- 강한 일관성: 모든 읽기 연산은 일관성 있는 값을 반환한다.
  - 모든 사본에 쓰기 연산의 결과가 반영될 때 까지 읽기/쓰기를 금지한다. 이 경우, 가용성이 저하될 수 있다. (쓰기/읽기 금지로 인해)
- 약한 일관성/최종 일관성: 읽기 연산은 일관성 있는 값을 반환하지 못할 수 있다.

## 데이터 파티션
- 데이터 파티셔닝은 데이터를 나누는 모든 개념
- DB 파티셔닝은 단일 데이터베이스에서 테이블을 나누는 것
- 샤딩은 분산 데이터베이스에서 데이터를 여러 노드로 나누는 것
- DB 파티셔닝, 샤딩은 데이터 파티션의 세부 종류

### 버저닝, 벡터 시계
- 여러 서버에 데이터를 복제해서 저장할 때, 동시성 이슈로 인해 일관성이 깨질 수 있다.
- 이를 해결하기 위해, 데이터에 각 서버와 버전을 매달아서 충돌을 판별할 수 있다.

## 복제 (replication)
- 복제란 네트워크로 연결된 장비에 복사본을 유지한다.
- 이를 통해 지역 latency 감소, HA, 읽기 스케일 아웃을 얻을 수 있다.

### 단일 리더 복제
- master slave 또는 active-passive 라고도 하며, 하나의 leader 가 쓰기 요청을 받아 follower 에게 전송한다.
- leader 의 failover 는 수동으로 할 수 있으며, follower 하나를 승격한다.
- 복제 지연: 어쩔 수 없이 최종적 일관성을 사용해야 한다.

### 다중 리더 복제 
- multi master 또는 active-active 라고도 하며 여러 leader 가 쓰기 요청을 받아 다른 leader 와 follower 에게 전송한다.
- 쓰기 충돌이 발생한다.


# 분산 Key-Value 스토리지의 장애 감지와 복구

## 1. 분산 Key-Value 스토리지의 장애 감지

### 주요 장애 시나리오
- **노드 장애**: 특정 노드가 다운되거나 응답을 중단.
- **네트워크 파티션**: 네트워크 연결 문제로 노드 간 통신이 단절.
- **데이터 불일치**: 데이터 복제본 간의 불일치 발생.

### 장애 감지 메커니즘
#### 1. Heartbeat (심장박동)
- 각 노드가 주기적으로 클러스터 내 다른 노드에 신호를 보내 상태를 알림.
- 신호가 일정 시간 내에 도착하지 않으면 해당 노드에 장애 발생으로 간주.
- **예**: Apache Cassandra는 **Gossip Protocol**을 사용해 상태 정보를 전파.

#### 2. Gossip Protocol
- 클러스터 내 노드 간 주기적으로 상태 정보를 공유하여 장애를 감지.
- 특정 노드에서 장애를 발견하면, 이를 다른 노드에 빠르게 전파.

#### 3. Timeout 기반 요청 모니터링
- 클라이언트 또는 다른 노드로부터의 요청 응답 시간이 설정된 **Timeout**을 초과하면 장애로 판단.
- **예**: DynamoDB는 네트워크 및 처리 시간 초과를 통해 노드 장애를 감지.

#### 4. 헬스 체크
- 각 노드에서 헬스 체크 API를 호출하거나, 관리 노드가 정기적으로 노드 상태를 확인.
- **예**: Redis Cluster는 **PING-PONG 메커니즘**을 통해 마스터와 슬레이브 노드 간 상태를 점검.

---

## 2. 분산 Key-Value 스토리지의 장애 복구

### 장애 복구 전략
#### 1. Replication (복제)
- 데이터를 여러 노드에 복제하여, 한 노드에서 장애가 발생하더라도 다른 노드에서 데이터 제공 가능.
- **Consistent Hashing**을 사용해 데이터가 균등하게 분산되며, 장애 시 인접 노드가 데이터를 복구.
- **예**:
  - Cassandra는 **Replication Factor**를 설정하여 데이터를 복제.
  - Redis Cluster는 **슬레이브 노드가 마스터를 대체**.

#### 2. Failover (장애 전환)
- 장애가 발생한 노드의 역할을 대체 노드로 자동 전환.
- **예**: Redis Cluster는 장애 발생 시 슬레이브 노드를 마스터로 승격.

#### 3. Hinted Handoff
- 데이터가 쓰여야 할 노드가 다운된 경우, 다른 노드가 데이터를 임시로 저장.
- 장애 노드가 복구되면 해당 데이터를 복구.
- **예**: Cassandra.

#### 4. Read Repair
- 데이터를 읽는 동안 복제본 간 데이터 불일치가 감지되면 자동으로 동기화.
- **예**: DynamoDB 및 Cassandra.

#### 5. Anti-Entropy Mechanism
- 노드 간 주기적으로 데이터를 비교하고 복구.
- **예**: Merkle Tree를 사용해 데이터 불일치를 감지하고 동기화.
- **예**: Cassandra, DynamoDB.

#### 6. Auto-scaling 및 재시작
- 클라우드 환경에서는 노드 장애 발생 시 자동으로 새로운 노드를 추가하거나, 장애 노드를 재시작.
- **예**: AWS DynamoDB의 서버리스 아키텍처는 노드 장애를 자동으로 복구.

#### 7. Quorum Consensus
- 쓰기/읽기 요청 시 다수의 복제본이 응답을 반환해야 성공으로 간주.
- 장애 노드가 존재해도 **Quorum(다수결)** 메커니즘을 통해 일관성을 유지.
- **예**: DynamoDB의 **Strong Consistency** 옵션.
